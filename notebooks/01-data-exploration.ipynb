{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Colorization Project - Data Exploration\n",
    "\n",
    "This notebook helps you explore and understand your dataset before training.\n",
    "\n",
    "## Objectives:\n",
    "- Analyze dataset structure and statistics\n",
    "- Visualize color distributions\n",
    "- Check image quality and formats\n",
    "- Identify potential issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "from utils import calculate_dataset_statistics, visualize_color_distribution\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data directories\n",
    "data_root = \"../data/raw\"\n",
    "train_dir = os.path.join(data_root, \"train\")\n",
    "val_dir = os.path.join(data_root, \"val\")\n",
    "test_dir = os.path.join(data_root, \"test\")\n",
    "\n",
    "# Check if directories exist\n",
    "directories = [train_dir, val_dir, test_dir]\n",
    "for directory in directories:\n",
    "    if os.path.exists(directory):\n",
    "        print(f\"✓ {directory} exists\")\n",
    "    else:\n",
    "        print(f\"✗ {directory} does not exist\")\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        print(f\"  Created {directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count images in each directory\n",
    "def count_images(directory):\n",
    "    \"\"\"Count image files in a directory.\"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        return 0\n",
    "    \n",
    "    image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']\n",
    "    count = 0\n",
    "    \n",
    "    for file in os.listdir(directory):\n",
    "        if any(file.lower().endswith(ext) for ext in image_extensions):\n",
    "            count += 1\n",
    "    \n",
    "    return count\n",
    "\n",
    "# Count images\n",
    "train_count = count_images(train_dir)\n",
    "val_count = count_images(val_dir)\n",
    "test_count = count_images(test_dir)\n",
    "total_count = train_count + val_count + test_count\n",
    "\n",
    "print(\"Dataset Statistics:\")\n",
    "print(f\"Training images: {train_count}\")\n",
    "print(f\"Validation images: {val_count}\")\n",
    "print(f\"Test images: {test_count}\")\n",
    "print(f\"Total images: {total_count}\")\n",
    "\n",
    "if total_count == 0:\n",
    "    print(\"\\n⚠️  No images found! Please add images to the data/raw/ directories.\")\n",
    "    print(\"   Refer to the README.md for instructions on obtaining datasets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize dataset distribution\n",
    "if total_count > 0:\n",
    "    # Create pie chart\n",
    "    sizes = [train_count, val_count, test_count]\n",
    "    labels = ['Train', 'Validation', 'Test']\n",
    "    colors = ['skyblue', 'lightcoral', 'lightgreen']\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    # Pie chart\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "    plt.title('Dataset Distribution')\n",
    "    \n",
    "    # Bar chart\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.bar(labels, sizes, color=colors)\n",
    "    plt.title('Image Counts by Split')\n",
    "    plt.ylabel('Number of Images')\n",
    "    \n",
    "    for i, v in enumerate(sizes):\n",
    "        plt.text(i, v + max(sizes)*0.01, str(v), ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Image Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze image properties\n",
    "def analyze_images(directory, max_samples=50):\n",
    "    \"\"\"Analyze image properties in a directory.\"\"\"\n",
    "    if not os.path.exists(directory) or count_images(directory) == 0:\n",
    "        return None\n",
    "    \n",
    "    image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']\n",
    "    image_files = []\n",
    "    \n",
    "    for file in os.listdir(directory):\n",
    "        if any(file.lower().endswith(ext) for ext in image_extensions):\n",
    "            image_files.append(os.path.join(directory, file))\n",
    "    \n",
    "    # Sample images if too many\n",
    "    if len(image_files) > max_samples:\n",
    "        image_files = np.random.choice(image_files, max_samples, replace=False)\n",
    "    \n",
    "    properties = {\n",
    "        'widths': [],\n",
    "        'heights': [],\n",
    "        'aspects': [],\n",
    "        'formats': [],\n",
    "        'sizes_mb': [],\n",
    "        'modes': []\n",
    "    }\n",
    "    \n",
    "    for img_path in image_files:\n",
    "        try:\n",
    "            with Image.open(img_path) as img:\n",
    "                properties['widths'].append(img.width)\n",
    "                properties['heights'].append(img.height)\n",
    "                properties['aspects'].append(img.width / img.height)\n",
    "                properties['formats'].append(img.format)\n",
    "                properties['modes'].append(img.mode)\n",
    "                \n",
    "            # File size\n",
    "            size_mb = os.path.getsize(img_path) / (1024 * 1024)\n",
    "            properties['sizes_mb'].append(size_mb)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_path}: {e}\")\n",
    "    \n",
    "    return properties\n",
    "\n",
    "# Analyze training images\n",
    "if train_count > 0:\n",
    "    print(\"Analyzing training images...\")\n",
    "    train_props = analyze_images(train_dir)\n",
    "    \n",
    "    if train_props:\n",
    "        print(f\"\\nTraining Set Analysis (sample of {len(train_props['widths'])} images):\")\n",
    "        print(f\"Average width: {np.mean(train_props['widths']):.0f}px\")\n",
    "        print(f\"Average height: {np.mean(train_props['heights']):.0f}px\")\n",
    "        print(f\"Average aspect ratio: {np.mean(train_props['aspects']):.2f}\")\n",
    "        print(f\"Average file size: {np.mean(train_props['sizes_mb']):.2f}MB\")\n",
    "        \n",
    "        # Most common format\n",
    "        format_counts = pd.Series(train_props['formats']).value_counts()\n",
    "        print(f\"Most common format: {format_counts.index[0]} ({format_counts.iloc[0]} files)\")\n",
    "        \n",
    "        # Most common mode\n",
    "        mode_counts = pd.Series(train_props['modes']).value_counts()\n",
    "        print(f\"Color modes: {dict(mode_counts)}\")\n",
    "else:\n",
    "    print(\"No training images to analyze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize image properties\n",
    "if train_count > 0 and train_props:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    \n",
    "    # Width distribution\n",
    "    axes[0, 0].hist(train_props['widths'], bins=20, alpha=0.7, color='blue')\n",
    "    axes[0, 0].set_title('Image Width Distribution')\n",
    "    axes[0, 0].set_xlabel('Width (pixels)')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    \n",
    "    # Height distribution\n",
    "    axes[0, 1].hist(train_props['heights'], bins=20, alpha=0.7, color='green')\n",
    "    axes[0, 1].set_title('Image Height Distribution')\n",
    "    axes[0, 1].set_xlabel('Height (pixels)')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    \n",
    "    # Aspect ratio distribution\n",
    "    axes[0, 2].hist(train_props['aspects'], bins=20, alpha=0.7, color='orange')\n",
    "    axes[0, 2].set_title('Aspect Ratio Distribution')\n",
    "    axes[0, 2].set_xlabel('Aspect Ratio (W/H)')\n",
    "    axes[0, 2].set_ylabel('Frequency')\n",
    "    \n",
    "    # File size distribution\n",
    "    axes[1, 0].hist(train_props['sizes_mb'], bins=20, alpha=0.7, color='red')\n",
    "    axes[1, 0].set_title('File Size Distribution')\n",
    "    axes[1, 0].set_xlabel('Size (MB)')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    \n",
    "    # Format distribution\n",
    "    format_counts = pd.Series(train_props['formats']).value_counts()\n",
    "    axes[1, 1].pie(format_counts.values, labels=format_counts.index, autopct='%1.1f%%')\n",
    "    axes[1, 1].set_title('Image Format Distribution')\n",
    "    \n",
    "    # Width vs Height scatter\n",
    "    axes[1, 2].scatter(train_props['widths'], train_props['heights'], alpha=0.6)\n",
    "    axes[1, 2].set_title('Width vs Height')\n",
    "    axes[1, 2].set_xlabel('Width (pixels)')\n",
    "    axes[1, 2].set_ylabel('Height (pixels)')\n",
    "    \n",
    "    # Add diagonal line for square images\n",
    "    max_dim = max(max(train_props['widths']), max(train_props['heights']))\n",
    "    axes[1, 2].plot([0, max_dim], [0, max_dim], 'r--', alpha=0.5, label='Square')\n",
    "    axes[1, 2].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sample Image Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample images\n",
    "def show_sample_images(directory, num_samples=8):\n",
    "    \"\"\"Display sample images from a directory.\"\"\"\n",
    "    if not os.path.exists(directory) or count_images(directory) == 0:\n",
    "        print(f\"No images found in {directory}\")\n",
    "        return\n",
    "    \n",
    "    image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']\n",
    "    image_files = []\n",
    "    \n",
    "    for file in os.listdir(directory):\n",
    "        if any(file.lower().endswith(ext) for ext in image_extensions):\n",
    "            image_files.append(os.path.join(directory, file))\n",
    "    \n",
    "    # Random sample\n",
    "    if len(image_files) > num_samples:\n",
    "        image_files = np.random.choice(image_files, num_samples, replace=False)\n",
    "    \n",
    "    # Calculate grid size\n",
    "    cols = min(4, len(image_files))\n",
    "    rows = (len(image_files) + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(15, 4*rows))\n",
    "    if rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    elif cols == 1:\n",
    "        axes = axes.reshape(-1, 1)\n",
    "    \n",
    "    for i, img_path in enumerate(image_files):\n",
    "        row = i // cols\n",
    "        col = i % cols\n",
    "        \n",
    "        try:\n",
    "            img = Image.open(img_path)\n",
    "            axes[row, col].imshow(img)\n",
    "            axes[row, col].set_title(f\"{os.path.basename(img_path)}\\n{img.size[0]}x{img.size[1]}\")\n",
    "            axes[row, col].axis('off')\n",
    "        except Exception as e:\n",
    "            axes[row, col].text(0.5, 0.5, f\"Error loading\\n{os.path.basename(img_path)}\", \n",
    "                              ha='center', va='center')\n",
    "            axes[row, col].set_xlim(0, 1)\n",
    "            axes[row, col].set_ylim(0, 1)\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(len(image_files), rows * cols):\n",
    "        row = i // cols\n",
    "        col = i % cols\n",
    "        axes[row, col].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Show sample training images\n",
    "if train_count > 0:\n",
    "    print(\"Sample Training Images:\")\n",
    "    show_sample_images(train_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Color Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze color distribution of sample images\n",
    "def analyze_colors(directory, num_samples=5):\n",
    "    \"\"\"Analyze color distribution of sample images.\"\"\"\n",
    "    if not os.path.exists(directory) or count_images(directory) == 0:\n",
    "        print(f\"No images found in {directory}\")\n",
    "        return\n",
    "    \n",
    "    image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']\n",
    "    image_files = []\n",
    "    \n",
    "    for file in os.listdir(directory):\n",
    "        if any(file.lower().endswith(ext) for ext in image_extensions):\n",
    "            image_files.append(os.path.join(directory, file))\n",
    "    \n",
    "    # Random sample\n",
    "    if len(image_files) > num_samples:\n",
    "        image_files = np.random.choice(image_files, num_samples, replace=False)\n",
    "    \n",
    "    for img_path in image_files:\n",
    "        try:\n",
    "            print(f\"\\nAnalyzing: {os.path.basename(img_path)}\")\n",
    "            visualize_color_distribution(img_path, title=os.path.basename(img_path))\n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing {img_path}: {e}\")\n",
    "\n",
    "# Analyze colors of sample images\n",
    "if train_count > 0:\n",
    "    print(\"Color Distribution Analysis:\")\n",
    "    analyze_colors(train_dir, num_samples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for potential issues\n",
    "def check_data_quality(directory):\n",
    "    \"\"\"Check for potential data quality issues.\"\"\"\n",
    "    if not os.path.exists(directory) or count_images(directory) == 0:\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nData Quality Check for {directory}:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']\n",
    "    issues = []\n",
    "    \n",
    "    image_files = []\n",
    "    for file in os.listdir(directory):\n",
    "        if any(file.lower().endswith(ext) for ext in image_extensions):\n",
    "            image_files.append(os.path.join(directory, file))\n",
    "    \n",
    "    small_images = 0\n",
    "    large_images = 0\n",
    "    corrupted_images = 0\n",
    "    grayscale_images = 0\n",
    "    \n",
    "    for img_path in image_files[:50]:  # Check first 50 images\n",
    "        try:\n",
    "            with Image.open(img_path) as img:\n",
    "                # Check size\n",
    "                if img.width < 64 or img.height < 64:\n",
    "                    small_images += 1\n",
    "                \n",
    "                if img.width > 4000 or img.height > 4000:\n",
    "                    large_images += 1\n",
    "                \n",
    "                # Check if grayscale\n",
    "                if img.mode in ['L', '1']:\n",
    "                    grayscale_images += 1\n",
    "                elif img.mode == 'RGB':\n",
    "                    # Check if RGB image is actually grayscale\n",
    "                    img_array = np.array(img)\n",
    "                    if len(img_array.shape) == 3:\n",
    "                        r, g, b = img_array[:,:,0], img_array[:,:,1], img_array[:,:,2]\n",
    "                        if np.allclose(r, g) and np.allclose(g, b):\n",
    "                            grayscale_images += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            corrupted_images += 1\n",
    "            print(f\"  Corrupted: {os.path.basename(img_path)}\")\n",
    "    \n",
    "    # Report issues\n",
    "    total_checked = min(50, len(image_files))\n",
    "    \n",
    "    print(f\"Images checked: {total_checked}\")\n",
    "    print(f\"Small images (<64px): {small_images}\")\n",
    "    print(f\"Large images (>4000px): {large_images}\")\n",
    "    print(f\"Grayscale images: {grayscale_images}\")\n",
    "    print(f\"Corrupted images: {corrupted_images}\")\n",
    "    \n",
    "    if small_images > 0:\n",
    "        print(\"⚠️  Warning: Small images may reduce training quality\")\n",
    "    \n",
    "    if large_images > 0:\n",
    "        print(\"⚠️  Warning: Large images will be resized, consider preprocessing\")\n",
    "    \n",
    "    if grayscale_images > total_checked * 0.5:\n",
    "        print(\"⚠️  Warning: Many grayscale images found - this is unusual for colorization\")\n",
    "    \n",
    "    if corrupted_images > 0:\n",
    "        print(\"❌ Error: Corrupted images found - please remove them\")\n",
    "\n",
    "# Check all splits\n",
    "for directory in [train_dir, val_dir, test_dir]:\n",
    "    if count_images(directory) > 0:\n",
    "        check_data_quality(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide recommendations based on analysis\n",
    "print(\"Dataset Recommendations:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if total_count == 0:\n",
    "    print(\"❌ NO IMAGES FOUND!\")\n",
    "    print(\"Please add color images to the data/raw/ directories.\")\n",
    "    print(\"\\nSuggested datasets:\")\n",
    "    print(\"- ImageNet subset\")\n",
    "    print(\"- COCO dataset\")\n",
    "    print(\"- Places365\")\n",
    "    print(\"- Your own collection of color photos\")\n",
    "    \n",
    "elif total_count < 1000:\n",
    "    print(\"⚠️  SMALL DATASET\")\n",
    "    print(f\"You have {total_count} images, which is quite small for deep learning.\")\n",
    "    print(\"Recommendations:\")\n",
    "    print(\"- Add more images if possible\")\n",
    "    print(\"- Use heavy data augmentation\")\n",
    "    print(\"- Consider transfer learning\")\n",
    "    print(\"- Use smaller model architectures\")\n",
    "    \n",
    "elif total_count < 10000:\n",
    "    print(\"✓ MODERATE DATASET\")\n",
    "    print(f\"You have {total_count} images, which is reasonable.\")\n",
    "    print(\"Recommendations:\")\n",
    "    print(\"- Use moderate data augmentation\")\n",
    "    print(\"- Monitor for overfitting\")\n",
    "    print(\"- Consider pre-trained backbones\")\n",
    "    \n",
    "else:\n",
    "    print(\"✅ LARGE DATASET\")\n",
    "    print(f\"You have {total_count} images, which is excellent!\")\n",
    "    print(\"Recommendations:\")\n",
    "    print(\"- You can use larger models\")\n",
    "    print(\"- Light to moderate augmentation should suffice\")\n",
    "    print(\"- Train for more epochs\")\n",
    "\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"1. If you need more images, check the README for dataset sources\")\n",
    "print(\"2. Run notebook 02_data_preprocessing.ipynb to prepare your data\")\n",
    "print(\"3. Experiment with augmentation in notebook 03_augmentation_experiments.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook analyzed your dataset structure, image properties, and quality. Use the insights to:\n",
    "\n",
    "1. **Dataset Size**: Determine if you need more data\n",
    "2. **Image Quality**: Identify and fix any issues\n",
    "3. **Color Distribution**: Understand the variety in your dataset\n",
    "4. **Next Steps**: Proceed with data preprocessing and model training\n",
    "\n",
    "Remember: Good data is crucial for good results in deep learning!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
