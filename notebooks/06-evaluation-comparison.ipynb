{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation and Model Comparison\n",
    "\n",
    "This notebook provides comprehensive evaluation and comparison between baseline and augmented models.\n",
    "\n",
    "## Objectives:\n",
    "- Load both baseline and augmented models\n",
    "- Perform detailed evaluation on test set\n",
    "- Compare performance metrics\n",
    "- Generate side-by-side result comparisons\n",
    "- Provide recommendations and insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "PyTorch version: 2.8.0+cpu\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "from model_architecture import create_model\n",
    "from data_preprocessing import DataPreprocessor\n",
    "from evaluation import ColorizationEvaluator, evaluate_single_image\n",
    "from utils import compare_images_side_by_side, create_model_summary\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Load Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison results will be saved to: ../results/comparisons\n",
      "\n",
      "Model Availability:\n",
      "Baseline model: ✅ Available\n",
      "Augmented model: ✅ Available\n",
      "\n",
      "Evaluation Data:\n",
      "Test images: 9\n",
      "Validation images: 9\n",
      "\n",
      "Using test set for evaluation (9 images)\n"
     ]
    }
   ],
   "source": [
    "# Load configuration\n",
    "config_path = \"../config/config.yaml\"\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Setup directories\n",
    "processed_dir = \"../data/processed\"\n",
    "test_dir = os.path.join(processed_dir, \"test\")\n",
    "val_dir = os.path.join(processed_dir, \"val\")\n",
    "\n",
    "baseline_model_dir = \"../models/baseline_model\"\n",
    "augmented_model_dir = \"../models/augmented_model\"\n",
    "comparison_results_dir = \"../results/comparisons\"\n",
    "\n",
    "# Create results directory\n",
    "os.makedirs(comparison_results_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Comparison results will be saved to: {comparison_results_dir}\")\n",
    "\n",
    "# Check available models\n",
    "baseline_model_path = os.path.join(baseline_model_dir, \"baseline_model_complete.pth\")\n",
    "augmented_model_path = os.path.join(augmented_model_dir, \"augmented_model_complete.pth\")\n",
    "\n",
    "baseline_available = os.path.exists(baseline_model_path)\n",
    "augmented_available = os.path.exists(augmented_model_path)\n",
    "\n",
    "print(f\"\\nModel Availability:\")\n",
    "print(f\"Baseline model: {'✅ Available' if baseline_available else '❌ Not found'}\")\n",
    "print(f\"Augmented model: {'✅ Available' if augmented_available else '❌ Not found'}\")\n",
    "\n",
    "if not baseline_available and not augmented_available:\n",
    "    print(\"\\n❌ No trained models found!\")\n",
    "    print(\"Please train models using notebooks 04 and 05 first.\")\n",
    "    raise SystemExit(\"Trained models required\")\n",
    "\n",
    "# Check evaluation data\n",
    "def count_images(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        return 0\n",
    "    return len([f for f in os.listdir(directory) \n",
    "               if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "\n",
    "test_count = count_images(test_dir)\n",
    "val_count = count_images(val_dir)\n",
    "\n",
    "print(f\"\\nEvaluation Data:\")\n",
    "print(f\"Test images: {test_count}\")\n",
    "print(f\"Validation images: {val_count}\")\n",
    "\n",
    "# Use test set if available, otherwise use validation set\n",
    "eval_dir = test_dir if test_count > 0 else val_dir\n",
    "eval_count = test_count if test_count > 0 else val_count\n",
    "eval_set_name = \"test\" if test_count > 0 else \"validation\"\n",
    "\n",
    "print(f\"\\nUsing {eval_set_name} set for evaluation ({eval_count} images)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Training Histories and Previous Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded baseline training history\n",
      "   Best validation loss: 0.4998\n",
      "   Training epochs: 11\n",
      "   Duration: 17.9 min\n",
      "\n",
      "✅ Loaded augmented training history\n",
      "   Best validation loss: 0.2086\n",
      "   Training epochs: 11\n",
      "   Duration: 38.6 min\n",
      "   Augmentation strategy: medium\n",
      "\n",
      "📊 Baseline quick evaluation available\n",
      "📊 Augmented quick evaluation available\n",
      "\n",
      "🎯 Ready for comprehensive model comparison!\n"
     ]
    }
   ],
   "source": [
    "# Load training histories\n",
    "baseline_history = None\n",
    "augmented_history = None\n",
    "\n",
    "baseline_history_path = \"../results/baseline/training_history.yaml\"\n",
    "augmented_history_path = \"../results/augmented/training_history.yaml\"\n",
    "\n",
    "if os.path.exists(baseline_history_path):\n",
    "    with open(baseline_history_path, 'r') as f:\n",
    "        baseline_history = yaml.safe_load(f)\n",
    "    print(f\"✅ Loaded baseline training history\")\n",
    "    print(f\"   Best validation loss: {baseline_history['best_val_loss']:.4f}\")\n",
    "    print(f\"   Training epochs: {baseline_history['total_epochs']}\")\n",
    "    print(f\"   Duration: {baseline_history['training_duration_seconds']/60:.1f} min\")\n",
    "\n",
    "if os.path.exists(augmented_history_path):\n",
    "    with open(augmented_history_path, 'r') as f:\n",
    "        augmented_history = yaml.safe_load(f)\n",
    "    print(f\"\\n✅ Loaded augmented training history\")\n",
    "    print(f\"   Best validation loss: {augmented_history['best_val_loss']:.4f}\")\n",
    "    print(f\"   Training epochs: {augmented_history['total_epochs']}\")\n",
    "    print(f\"   Duration: {augmented_history['training_duration_seconds']/60:.1f} min\")\n",
    "    print(f\"   Augmentation strategy: {augmented_history.get('augmentation_strategy', 'N/A')}\")\n",
    "\n",
    "# Load quick evaluation results if available\n",
    "baseline_eval = None\n",
    "augmented_eval = None\n",
    "\n",
    "baseline_eval_path = \"../results/baseline/quick_evaluation.yaml\"\n",
    "augmented_eval_path = \"../results/augmented/quick_evaluation.yaml\"\n",
    "\n",
    "if os.path.exists(baseline_eval_path):\n",
    "    with open(baseline_eval_path, 'r') as f:\n",
    "        baseline_eval = yaml.unsafe_load(f)\n",
    "    print(f\"\\n📊 Baseline quick evaluation available\")\n",
    "    \n",
    "if os.path.exists(augmented_eval_path):\n",
    "    with open(augmented_eval_path, 'r') as f:\n",
    "        augmented_eval = yaml.unsafe_load(f)\n",
    "    print(f\"📊 Augmented quick evaluation available\")\n",
    "\n",
    "print(f\"\\n🎯 Ready for comprehensive model comparison!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Initialize Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "Loading baseline model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:model_architecture:Initialized unet model\n",
      "INFO:model_architecture:Model created with 31,036,546 trainable parameters\n",
      "INFO:model_architecture:Model size: 118.44 MB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Baseline model loaded successfully\n",
      "   Parameters: 31,036,546\n",
      "   Architecture: unet\n",
      "\n",
      "Loading augmented model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:model_architecture:Initialized unet model\n",
      "INFO:model_architecture:Model created with 31,036,546 trainable parameters\n",
      "INFO:model_architecture:Model size: 118.44 MB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Augmented model loaded successfully\n",
      "   Parameters: 31,036,546\n",
      "   Architecture: unet\n",
      "   Augmentation: medium\n",
      "\n",
      "✅ Both models loaded - comparison possible!\n"
     ]
    }
   ],
   "source": [
    "# Initialize device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load baseline model\n",
    "baseline_model = None\n",
    "if baseline_available:\n",
    "    print(\"\\nLoading baseline model...\")\n",
    "    try:\n",
    "        baseline_model = create_model(config_path)\n",
    "        baseline_checkpoint = torch.load(baseline_model_path, map_location=device)\n",
    "        baseline_model.load_state_dict(baseline_checkpoint['model_state_dict'])\n",
    "        baseline_model = baseline_model.to(device)\n",
    "        baseline_model.eval()\n",
    "        print(f\"✅ Baseline model loaded successfully\")\n",
    "        print(f\"   Parameters: {baseline_model.count_parameters():,}\")\n",
    "        print(f\"   Architecture: {baseline_checkpoint.get('model_architecture', 'Unknown')}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading baseline model: {e}\")\n",
    "        baseline_model = None\n",
    "        baseline_available = False\n",
    "\n",
    "# Load augmented model\n",
    "augmented_model = None\n",
    "if augmented_available:\n",
    "    print(\"\\nLoading augmented model...\")\n",
    "    try:\n",
    "        augmented_model = create_model(config_path)\n",
    "        augmented_checkpoint = torch.load(augmented_model_path, map_location=device)\n",
    "        augmented_model.load_state_dict(augmented_checkpoint['model_state_dict'])\n",
    "        augmented_model = augmented_model.to(device)\n",
    "        augmented_model.eval()\n",
    "        print(f\"✅ Augmented model loaded successfully\")\n",
    "        print(f\"   Parameters: {augmented_model.count_parameters():,}\")\n",
    "        print(f\"   Architecture: {augmented_checkpoint.get('model_architecture', 'Unknown')}\")\n",
    "        print(f\"   Augmentation: {augmented_checkpoint.get('augmentation_strategy', 'Unknown')}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading augmented model: {e}\")\n",
    "        augmented_model = None\n",
    "        augmented_available = False\n",
    "\n",
    "# Check if we can proceed with comparison\n",
    "can_compare = baseline_available and augmented_available\n",
    "print(f\"\\n{'✅ Both models loaded - comparison possible!' if can_compare else '⚠️ Only one model available - limited evaluation'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Evaluation Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:data_preprocessing:Found 9 images in ../data/processed\\test\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating evaluation data loader...\n",
      "✅ Evaluation data loader created\n",
      "   Dataset: test set\n",
      "   Images: 9\n",
      "   Batches: 1\n",
      "   Batch shape: Ltorch.Size([9, 1, 256, 256]), ABtorch.Size([9, 2, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# Create data loader for evaluation\n",
    "print(\"Creating evaluation data loader...\")\n",
    "\n",
    "try:\n",
    "    preprocessor = DataPreprocessor(config_path)\n",
    "    \n",
    "    if eval_count > 0:\n",
    "        # Create single data loader for the evaluation set\n",
    "        from torch.utils.data import DataLoader\n",
    "        from data_preprocessing import ColorDataset\n",
    "        \n",
    "        eval_dataset = ColorDataset(\n",
    "            eval_dir,\n",
    "            image_size=tuple(config['data']['input_size'])\n",
    "        )\n",
    "        \n",
    "        eval_loader = DataLoader(\n",
    "            eval_dataset,\n",
    "            batch_size=config['data']['batch_size'],\n",
    "            shuffle=False,  # Don't shuffle for consistent evaluation\n",
    "            num_workers=config['data']['num_workers'],\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ Evaluation data loader created\")\n",
    "        print(f\"   Dataset: {eval_set_name} set\")\n",
    "        print(f\"   Images: {len(eval_dataset)}\")\n",
    "        print(f\"   Batches: {len(eval_loader)}\")\n",
    "        \n",
    "        # Test data loading\n",
    "        for L, AB, filenames in eval_loader:\n",
    "            print(f\"   Batch shape: L{L.shape}, AB{AB.shape}\")\n",
    "            break\n",
    "            \n",
    "    else:\n",
    "        print(f\"❌ No evaluation data available\")\n",
    "        eval_loader = None\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error creating evaluation data loader: {e}\")\n",
    "    eval_loader = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comprehensive Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:evaluation:Starting model evaluation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing comprehensive model evaluation...\n",
      "This may take several minutes...\n",
      "\n",
      "📊 Evaluating baseline model...\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive evaluation of both models\n",
    "if eval_loader is not None:\n",
    "    print(\"Performing comprehensive model evaluation...\")\n",
    "    print(\"This may take several minutes...\")\n",
    "    \n",
    "    evaluator = ColorizationEvaluator(config_path)\n",
    "    \n",
    "    baseline_results = None\n",
    "    augmented_results = None\n",
    "    \n",
    "    # Evaluate baseline model\n",
    "    if baseline_model is not None:\n",
    "        print(f\"\\n📊 Evaluating baseline model...\")\n",
    "        try:\n",
    "            baseline_results = evaluator.evaluate_model(\n",
    "                baseline_model,\n",
    "                eval_loader,\n",
    "                save_results=False\n",
    "            )\n",
    "            print(f\"✅ Baseline evaluation completed\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error evaluating baseline model: {e}\")\n",
    "    \n",
    "    # Evaluate augmented model\n",
    "    if augmented_model is not None:\n",
    "        print(f\"\\n📊 Evaluating augmented model...\")\n",
    "        try:\n",
    "            augmented_results = evaluator.evaluate_model(\n",
    "                augmented_model,\n",
    "                eval_loader,\n",
    "                save_results=False\n",
    "            )\n",
    "            print(f\"✅ Augmented evaluation completed\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error evaluating augmented model: {e}\")\n",
    "    \n",
    "    print(f\"\\n🎉 Comprehensive evaluation completed!\")\n",
    "\n",
    "else:\n",
    "    print(\"⚠️ Skipping comprehensive evaluation - no evaluation data available\")\n",
    "    baseline_results = None\n",
    "    augmented_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Detailed Comparison Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Skipping detailed comparison - evaluation results not available\n"
     ]
    }
   ],
   "source": [
    "# Generate detailed comparison report\n",
    "if baseline_results and augmented_results:\n",
    "    print(\"Generating detailed comparison report...\")\n",
    "    \n",
    "    # Create comprehensive comparison\n",
    "    comparison_data = {\n",
    "        'evaluation_date': datetime.now().isoformat(),\n",
    "        'evaluation_dataset': eval_set_name,\n",
    "        'samples_evaluated': eval_count,\n",
    "        'models_compared': ['baseline', 'augmented'],\n",
    "        \n",
    "        'baseline_model': {\n",
    "            'training_augmentation': False,\n",
    "            'best_training_val_loss': baseline_history['best_val_loss'] if baseline_history else None,\n",
    "            'training_epochs': baseline_history['total_epochs'] if baseline_history else None,\n",
    "            'evaluation_metrics': baseline_results\n",
    "        },\n",
    "        \n",
    "        'augmented_model': {\n",
    "            'training_augmentation': True,\n",
    "            'augmentation_strategy': augmented_history.get('augmentation_strategy') if augmented_history else None,\n",
    "            'best_training_val_loss': augmented_history['best_val_loss'] if augmented_history else None,\n",
    "            'training_epochs': augmented_history['total_epochs'] if augmented_history else None,\n",
    "            'evaluation_metrics': augmented_results\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Calculate improvements\n",
    "    improvements = {}\n",
    "    for metric in ['psnr_mean', 'ssim_mean', 'mse_mean', 'mae_mean']:\n",
    "        if metric in baseline_results and metric in augmented_results:\n",
    "            baseline_val = baseline_results[metric]\n",
    "            augmented_val = augmented_results[metric]\n",
    "            \n",
    "            if metric in ['psnr_mean', 'ssim_mean']:  # Higher is better\n",
    "                improvement = augmented_val - baseline_val\n",
    "                improvement_pct = (improvement / baseline_val) * 100\n",
    "            else:  # Lower is better (mse, mae)\n",
    "                improvement = baseline_val - augmented_val\n",
    "                improvement_pct = (improvement / baseline_val) * 100\n",
    "            \n",
    "            improvements[metric] = {\n",
    "                'absolute_improvement': float(improvement),\n",
    "                'percentage_improvement': float(improvement_pct),\n",
    "                'baseline_value': float(baseline_val),\n",
    "                'augmented_value': float(augmented_val)\n",
    "            }\n",
    "    \n",
    "    comparison_data['improvements'] = improvements\n",
    "    \n",
    "    # Overall assessment\n",
    "    significant_improvements = sum(1 for imp in improvements.values() if abs(imp['percentage_improvement']) > 1)\n",
    "    positive_improvements = sum(1 for imp in improvements.values() if imp['percentage_improvement'] > 0)\n",
    "    \n",
    "    if positive_improvements >= 3:  # Most metrics improved\n",
    "        overall_result = \"significant_improvement\"\n",
    "    elif positive_improvements >= 2:\n",
    "        overall_result = \"moderate_improvement\"\n",
    "    elif positive_improvements >= 1:\n",
    "        overall_result = \"slight_improvement\"\n",
    "    else:\n",
    "        overall_result = \"no_improvement\"\n",
    "    \n",
    "    comparison_data['overall_result'] = overall_result\n",
    "    \n",
    "    # Save comparison report\n",
    "    comparison_path = os.path.join(comparison_results_dir, 'detailed_comparison.yaml')\n",
    "    with open(comparison_path, 'w') as f:\n",
    "        yaml.dump(comparison_data, f, default_flow_style=False)\n",
    "    \n",
    "    print(f\"✅ Detailed comparison report saved to: {comparison_path}\")\n",
    "    \n",
    "    # Display key results\n",
    "    print(f\"\\n📈 DETAILED COMPARISON RESULTS\")\n",
    "    print(f\"=\" * 50)\n",
    "    print(f\"Overall Result: {overall_result.replace('_', ' ').title()}\")\n",
    "    print(f\"Metrics with positive improvement: {positive_improvements}/4\")\n",
    "    \n",
    "    print(f\"\\n📊 Metric-by-Metric Comparison:\")\n",
    "    for metric, data in improvements.items():\n",
    "        metric_name = metric.replace('_mean', '').upper()\n",
    "        status = \"✅\" if data['percentage_improvement'] > 1 else \"➖\" if data['percentage_improvement'] > -1 else \"❌\"\n",
    "        print(f\"  {metric_name:<4}: {data['percentage_improvement']:+.2f}% improvement {status}\")\n",
    "\n",
    "else:\n",
    "    print(\"⚠️ Skipping detailed comparison - evaluation results not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Comparison Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison visualizations\n",
    "if baseline_results and augmented_results:\n",
    "    print(\"Creating comparison visualizations...\")\n",
    "    \n",
    "    # Metrics comparison bar chart\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    metrics_to_plot = ['psnr_mean', 'ssim_mean', 'mse_mean', 'mae_mean']\n",
    "    metric_names = ['PSNR (dB)', 'SSIM', 'MSE', 'MAE']\n",
    "    \n",
    "    for i, (metric, name) in enumerate(zip(metrics_to_plot, metric_names)):\n",
    "        row, col = i // 2, i % 2\n",
    "        \n",
    "        if metric in baseline_results and metric in augmented_results:\n",
    "            baseline_val = baseline_results[metric]\n",
    "            augmented_val = augmented_results[metric]\n",
    "            \n",
    "            values = [baseline_val, augmented_val]\n",
    "            labels = ['Baseline\\n(No Augmentation)', 'Augmented\\n(With Augmentation)']\n",
    "            colors = ['lightblue', 'lightcoral']\n",
    "            \n",
    "            bars = axes[row, col].bar(labels, values, color=colors)\n",
    "            axes[row, col].set_title(f'{name} Comparison')\n",
    "            axes[row, col].set_ylabel(name)\n",
    "            \n",
    "            # Add value labels on bars\n",
    "            for bar, value in zip(bars, values):\n",
    "                height = bar.get_height()\n",
    "                axes[row, col].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                                   f'{value:.4f}', ha='center', va='bottom')\n",
    "            \n",
    "            # Add improvement annotation\n",
    "            if metric in improvements:\n",
    "                imp_pct = improvements[metric]['percentage_improvement']\n",
    "                color = 'green' if imp_pct > 0 else 'red'\n",
    "                axes[row, col].text(0.5, max(values) * 1.1, f'{imp_pct:+.2f}% change',\n",
    "                                   ha='center', va='center', color=color, weight='bold',\n",
    "                                   transform=axes[row, col].transData)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save metrics comparison\n",
    "    metrics_comparison_path = os.path.join(comparison_results_dir, 'metrics_comparison.png')\n",
    "    plt.savefig(metrics_comparison_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"✅ Metrics comparison saved to: {metrics_comparison_path}\")\n",
    "    \n",
    "    # Create improvement radar chart\n",
    "    if len(improvements) >= 3:\n",
    "        print(\"\\nCreating improvement radar chart...\")\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 8), subplot_kw=dict(projection='polar'))\n",
    "        \n",
    "        # Prepare data for radar chart\n",
    "        metrics = list(improvements.keys())\n",
    "        values = [improvements[m]['percentage_improvement'] for m in metrics]\n",
    "        \n",
    "        # Add first value at the end to close the circle\n",
    "        values += values[:1]\n",
    "        \n",
    "        # Calculate angles for each metric\n",
    "        angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()\n",
    "        angles += angles[:1]\n",
    "        \n",
    "        # Plot\n",
    "        ax.plot(angles, values, 'o-', linewidth=2, label='Improvement %')\n",
    "        ax.fill(angles, values, alpha=0.25)\n",
    "        \n",
    "        # Add labels\n",
    "        metric_labels = [m.replace('_mean', '').upper() for m in metrics]\n",
    "        ax.set_xticks(angles[:-1])\n",
    "        ax.set_xticklabels(metric_labels)\n",
    "        \n",
    "        # Add grid lines at key improvement levels\n",
    "        ax.set_ylim(-10, 10)\n",
    "        ax.set_yticks([-5, 0, 5])\n",
    "        ax.set_yticklabels(['-5%', '0%', '+5%'])\n",
    "        ax.grid(True)\n",
    "        \n",
    "        # Add zero line\n",
    "        ax.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "        \n",
    "        plt.title('Improvement Percentage by Metric\\n(Augmented vs Baseline)', \n",
    "                 size=16, weight='bold', pad=20)\n",
    "        \n",
    "        # Save radar chart\n",
    "        radar_path = os.path.join(comparison_results_dir, 'improvement_radar.png')\n",
    "        plt.savefig(radar_path, dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"✅ Improvement radar chart saved to: {radar_path}\")\n",
    "\n",
    "else:\n",
    "    print(\"⚠️ Skipping visualizations - comparison results not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Side-by-Side Result Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate side-by-side colorization results\n",
    "if baseline_model and augmented_model and eval_loader:\n",
    "    print(\"Generating side-by-side colorization comparisons...\")\n",
    "    \n",
    "    try:\n",
    "        # Get sample images for comparison\n",
    "        sample_results = []\n",
    "        max_samples = 6\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (L, AB, filenames) in enumerate(eval_loader):\n",
    "                if len(sample_results) >= max_samples:\n",
    "                    break\n",
    "                \n",
    "                L_gpu = L.to(device)\n",
    "                \n",
    "                # Get predictions from both models\n",
    "                baseline_pred = baseline_model(L_gpu).cpu()\n",
    "                augmented_pred = augmented_model(L_gpu).cpu()\n",
    "                \n",
    "                # Convert to RGB\n",
    "                baseline_rgb = evaluator.lab_to_rgb(L, baseline_pred)\n",
    "                augmented_rgb = evaluator.lab_to_rgb(L, augmented_pred)\n",
    "                target_rgb = evaluator.lab_to_rgb(L, AB)\n",
    "                \n",
    "                # Store results\n",
    "                for i in range(min(2, len(filenames))):\n",
    "                    if len(sample_results) >= max_samples:\n",
    "                        break\n",
    "                        \n",
    "                    sample_results.append({\n",
    "                        'filename': filenames[i],\n",
    "                        'input_L': (L[i, 0].numpy() + 1) / 2,  # Denormalize for display\n",
    "                        'baseline_pred': baseline_rgb[i],\n",
    "                        'augmented_pred': augmented_rgb[i],\n",
    "                        'target': target_rgb[i]\n",
    "                    })\n",
    "        \n",
    "        # Create comparison visualization\n",
    "        if sample_results:\n",
    "            num_samples = len(sample_results)\n",
    "            fig, axes = plt.subplots(4, num_samples, figsize=(4*num_samples, 16))\n",
    "            \n",
    "            if num_samples == 1:\n",
    "                axes = axes.reshape(4, 1)\n",
    "            \n",
    "            for i, result in enumerate(sample_results):\n",
    "                # Input (grayscale)\n",
    "                axes[0, i].imshow(result['input_L'], cmap='gray')\n",
    "                axes[0, i].set_title(f'Input (Grayscale)\\n{result[\"filename\"][:20]}...', fontsize=10)\n",
    "                axes[0, i].axis('off')\n",
    "                \n",
    "                # Baseline prediction\n",
    "                axes[1, i].imshow(result['baseline_pred'])\n",
    "                axes[1, i].set_title('Baseline Prediction\\n(No Augmentation)', fontsize=10)\n",
    "                axes[1, i].axis('off')\n",
    "                \n",
    "                # Augmented prediction\n",
    "                axes[2, i].imshow(result['augmented_pred'])\n",
    "                axes[2, i].set_title('Augmented Prediction\\n(With Augmentation)', fontsize=10)\n",
    "                axes[2, i].axis('off')\n",
    "                \n",
    "                # Ground truth\n",
    "                axes[3, i].imshow(result['target'])\n",
    "                axes[3, i].set_title('Ground Truth', fontsize=10)\n",
    "                axes[3, i].axis('off')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save side-by-side comparison\n",
    "            sidebyside_path = os.path.join(comparison_results_dir, 'side_by_side_comparison.png')\n",
    "            plt.savefig(sidebyside_path, dpi=150, bbox_inches='tight')\n",
    "            plt.show()\n",
    "            \n",
    "            print(f\"✅ Side-by-side comparison saved to: {sidebyside_path}\")\n",
    "            \n",
    "            # Calculate per-image metrics for these samples\n",
    "            print(f\"\\n📊 Per-Sample Comparison:\")\n",
    "            for i, result in enumerate(sample_results):\n",
    "                baseline_psnr = evaluator.calculate_psnr(result['baseline_pred'], result['target'])\n",
    "                augmented_psnr = evaluator.calculate_psnr(result['augmented_pred'], result['target'])\n",
    "                \n",
    "                baseline_ssim = evaluator.calculate_ssim(result['baseline_pred'], result['target'])\n",
    "                augmented_ssim = evaluator.calculate_ssim(result['augmented_pred'], result['target'])\n",
    "                \n",
    "                psnr_diff = augmented_psnr - baseline_psnr\n",
    "                ssim_diff = augmented_ssim - baseline_ssim\n",
    "                \n",
    "                print(f\"  Sample {i+1}: PSNR {psnr_diff:+.2f} dB, SSIM {ssim_diff:+.4f}\")\n",
    "        \n",
    "        else:\n",
    "            print(\"No sample results generated\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error generating side-by-side comparison: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"⚠️ Skipping side-by-side comparison - models or data not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training Progress Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'baseline_history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Compare training progress between models\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mbaseline_history\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m augmented_history:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating training progress comparison...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'baseline_history' is not defined"
     ]
    }
   ],
   "source": [
    "# Compare training progress between models\n",
    "if baseline_history and augmented_history:\n",
    "    print(\"Creating training progress comparison...\")\n",
    "    \n",
    "    try:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "        # Training loss comparison\n",
    "        baseline_epochs = range(1, len(baseline_history['train_losses']) + 1)\n",
    "        augmented_epochs = range(1, len(augmented_history['train_losses']) + 1)\n",
    "        \n",
    "        axes[0].plot(baseline_epochs, baseline_history['train_losses'], \n",
    "                    'b-', label='Baseline (No Aug)', linewidth=2, alpha=0.8)\n",
    "        axes[0].plot(augmented_epochs, augmented_history['train_losses'], \n",
    "                    'r-', label='Augmented (With Aug)', linewidth=2, alpha=0.8)\n",
    "        \n",
    "        axes[0].set_xlabel('Epoch')\n",
    "        axes[0].set_ylabel('Training Loss')\n",
    "        axes[0].set_title('Training Loss Comparison')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Validation loss comparison\n",
    "        axes[1].plot(baseline_epochs, baseline_history['val_losses'], \n",
    "                    'b-', label='Baseline (No Aug)', linewidth=2, alpha=0.8)\n",
    "        axes[1].plot(augmented_epochs, augmented_history['val_losses'], \n",
    "                    'r-', label='Augmented (With Aug)', linewidth=2, alpha=0.8)\n",
    "        \n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].set_ylabel('Validation Loss')\n",
    "        axes[1].set_title('Validation Loss Comparison')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add best loss markers\n",
    "        baseline_best_epoch = np.argmin(baseline_history['val_losses']) + 1\n",
    "        augmented_best_epoch = np.argmin(augmented_history['val_losses']) + 1\n",
    "        \n",
    "        axes[1].scatter(baseline_best_epoch, baseline_history['best_val_loss'], \n",
    "                       color='blue', s=100, zorder=5, marker='*')\n",
    "        axes[1].scatter(augmented_best_epoch, augmented_history['best_val_loss'], \n",
    "                       color='red', s=100, zorder=5, marker='*')\n",
    "        \n",
    "        # Annotations\n",
    "        axes[1].annotate(f'Baseline Best\\n{baseline_history[\"best_val_loss\"]:.4f}',\n",
    "                        xy=(baseline_best_epoch, baseline_history['best_val_loss']),\n",
    "                        xytext=(10, 10), textcoords='offset points', fontsize=9,\n",
    "                        bbox=dict(boxstyle='round,pad=0.3', facecolor='lightblue', alpha=0.8))\n",
    "        \n",
    "        axes[1].annotate(f'Augmented Best\\n{augmented_history[\"best_val_loss\"]:.4f}',\n",
    "                        xy=(augmented_best_epoch, augmented_history['best_val_loss']),\n",
    "                        xytext=(10, -30), textcoords='offset points', fontsize=9,\n",
    "                        bbox=dict(boxstyle='round,pad=0.3', facecolor='lightcoral', alpha=0.8))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save training comparison\n",
    "        training_comparison_path = os.path.join(comparison_results_dir, 'training_progress_comparison.png')\n",
    "        plt.savefig(training_comparison_path, dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"✅ Training progress comparison saved to: {training_comparison_path}\")\n",
    "        \n",
    "        # Training statistics comparison\n",
    "        print(f\"\\n📊 Training Statistics Comparison:\")\n",
    "        print(f\"\" + \"=\" * 50)\n",
    "        \n",
    "        training_stats = pd.DataFrame({\n",
    "            'Metric': [\n",
    "                'Training Duration (min)',\n",
    "                'Total Epochs',\n",
    "                'Best Validation Loss',\n",
    "                'Final Training Loss',\n",
    "                'Final Validation Loss'\n",
    "            ],\n",
    "            'Baseline': [\n",
    "                f\"{baseline_history['training_duration_seconds']/60:.1f}\",\n",
    "                baseline_history['total_epochs'],\n",
    "                f\"{baseline_history['best_val_loss']:.4f}\",\n",
    "                f\"{baseline_history['final_train_loss']:.4f}\",\n",
    "                f\"{baseline_history['final_val_loss']:.4f}\"\n",
    "            ],\n",
    "            'Augmented': [\n",
    "                f\"{augmented_history['training_duration_seconds']/60:.1f}\",\n",
    "                augmented_history['total_epochs'],\n",
    "                f\"{augmented_history['best_val_loss']:.4f}\",\n",
    "                f\"{augmented_history['final_train_loss']:.4f}\",\n",
    "                f\"{augmented_history['final_val_loss']:.4f}\"\n",
    "            ]\n",
    "        })\n",
    "        \n",
    "        print(training_stats.to_string(index=False))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error creating training progress comparison: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"⚠️ Training histories not available for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Final Report and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL EVALUATION REPORT\n",
      "============================================================\n",
      "Models Evaluated:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'baseline_available' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Model availability summary\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModels Evaluated:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Baseline Model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m✅ Available\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mbaseline_available\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m❌ Not Available\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Augmented Model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m✅ Available\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39maugmented_available\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m❌ Not Available\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m baseline_results \u001b[38;5;129;01mand\u001b[39;00m augmented_results:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'baseline_available' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate final comprehensive report\n",
    "print(\"FINAL EVALUATION REPORT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Model availability summary\n",
    "print(f\"Models Evaluated:\")\n",
    "print(f\"  Baseline Model: {'✅ Available' if baseline_available else '❌ Not Available'}\")\n",
    "print(f\"  Augmented Model: {'✅ Available' if augmented_available else '❌ Not Available'}\")\n",
    "\n",
    "if baseline_results and augmented_results:\n",
    "    print(f\"\\n📊 COMPREHENSIVE EVALUATION RESULTS:\")\n",
    "    print(f\"  Evaluation Dataset: {eval_set_name} set ({eval_count} images)\")\n",
    "    print(f\"  Overall Result: {comparison_data['overall_result'].replace('_', ' ').title()}\")\n",
    "    \n",
    "    # Key improvements\n",
    "    print(f\"\\n🎯 KEY IMPROVEMENTS:\")\n",
    "    for metric, data in improvements.items():\n",
    "        metric_name = metric.replace('_mean', '').upper()\n",
    "        if data['percentage_improvement'] > 1:\n",
    "            print(f\"  ✅ {metric_name}: +{data['percentage_improvement']:.2f}% improvement\")\n",
    "        elif data['percentage_improvement'] < -1:\n",
    "            print(f\"  ❌ {metric_name}: {data['percentage_improvement']:.2f}% regression\")\n",
    "        else:\n",
    "            print(f\"  ➖ {metric_name}: {data['percentage_improvement']:.2f}% (minimal change)\")\n",
    "    \n",
    "    # Best performer\n",
    "    better_model = \"Augmented\" if comparison_data['overall_result'] in ['significant_improvement', 'moderate_improvement'] else \"Baseline\"\n",
    "    print(f\"\\n🏆 RECOMMENDED MODEL: {better_model} Model\")\n",
    "\n",
    "else:\n",
    "    print(f\"\\n⚠️ Comprehensive comparison not available\")\n",
    "    print(f\"  Missing evaluation results for detailed comparison\")\n",
    "\n",
    "# Files generated\n",
    "print(f\"\\n📁 FILES GENERATED:\")\n",
    "generated_files = [\n",
    "    'detailed_comparison.yaml',\n",
    "    'metrics_comparison.png',\n",
    "    'improvement_radar.png',\n",
    "    'side_by_side_comparison.png',\n",
    "    'training_progress_comparison.png'\n",
    "]\n",
    "\n",
    "for file in generated_files:\n",
    "    file_path = os.path.join(comparison_results_dir, file)\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"  ✅ {file}\")\n",
    "    else:\n",
    "        print(f\"  ❌ {file} (not created)\")\n",
    "\n",
    "# Recommendations\n",
    "print(f\"\\n💡 RECOMMENDATIONS:\")\n",
    "\n",
    "if baseline_results and augmented_results:\n",
    "    if comparison_data['overall_result'] == 'significant_improvement':\n",
    "        print(f\"  🎉 Data augmentation was highly successful!\")\n",
    "        print(f\"  📈 Use the augmented model for production\")\n",
    "        print(f\"  🔧 Current augmentation strategy is working well\")\n",
    "        print(f\"  📊 Consider fine-tuning augmentation parameters for even better results\")\n",
    "    \n",
    "    elif comparison_data['overall_result'] == 'moderate_improvement':\n",
    "        print(f\"  ✅ Data augmentation provided modest improvements\")\n",
    "        print(f\"  📈 Use the augmented model, but gains are moderate\")\n",
    "        print(f\"  🔧 Consider experimenting with different augmentation strategies\")\n",
    "        print(f\"  📊 Additional techniques (transfer learning, architecture changes) may help\")\n",
    "    \n",
    "    elif comparison_data['overall_result'] == 'slight_improvement':\n",
    "        print(f\"  ➖ Data augmentation provided minimal improvement\")\n",
    "        print(f\"  🤔 Either model can be used - difference is small\")\n",
    "        print(f\"  🔧 Try different augmentation approaches or strategies\")\n",
    "        print(f\"  📊 Focus on other improvements (model architecture, training time)\")\n",
    "    \n",
    "    else:  # no_improvement\n",
    "        print(f\"  ⚠️ Data augmentation did not improve performance\")\n",
    "        print(f\"  📈 Use the baseline model (simpler and faster)\")\n",
    "        print(f\"  🔧 Current augmentation strategy may not suit your dataset\")\n",
    "        print(f\"  📊 Consider: less aggressive augmentation, different techniques, or focus on other improvements\")\n",
    "\n",
    "else:\n",
    "    print(f\"  📝 Complete both baseline and augmented training for full comparison\")\n",
    "    print(f\"  🔄 Run notebooks 04 and 05 if not already completed\")\n",
    "    print(f\"  📊 Ensure adequate evaluation data is available\")\n",
    "\n",
    "# Next steps\n",
    "print(f\"\\n🎯 NEXT STEPS:\")\n",
    "print(f\"  1. 🖥️ Test models interactively with notebook 07_gui_demo.ipynb\")\n",
    "print(f\"  2. 📊 Use the best model for your specific colorization needs\")\n",
    "print(f\"  3. 🔧 Consider additional improvements based on results\")\n",
    "print(f\"  4. 📝 Document your findings for future reference\")\n",
    "\n",
    "# Save final report\n",
    "final_report = {\n",
    "    'report_date': datetime.now().isoformat(),\n",
    "    'models_evaluated': {\n",
    "        'baseline_available': baseline_available,\n",
    "        'augmented_available': augmented_available\n",
    "    },\n",
    "    'evaluation_summary': comparison_data if 'comparison_data' in locals() else None,\n",
    "    'recommendations': {\n",
    "        'recommended_model': better_model if 'better_model' in locals() else 'Unknown',\n",
    "        'overall_result': comparison_data['overall_result'] if 'comparison_data' in locals() else 'Unknown'\n",
    "    },\n",
    "    'files_generated': [f for f in generated_files if os.path.exists(os.path.join(comparison_results_dir, f))]\n",
    "}\n",
    "\n",
    "final_report_path = os.path.join(comparison_results_dir, 'final_evaluation_report.yaml')\n",
    "with open(final_report_path, 'w') as f:\n",
    "    yaml.dump(final_report, f, default_flow_style=False)\n",
    "\n",
    "print(f\"\\n📋 Final report saved to: {final_report_path}\")\n",
    "print(f\"\\n🎉 COMPREHENSIVE EVALUATION COMPLETED!\")\n",
    "print(f\"Ready to use your trained models for image colorization! 🎨\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
