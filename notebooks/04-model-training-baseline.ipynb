{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model Training (No Augmentation)\n",
    "\n",
    "This notebook trains a colorization model without data augmentation to establish a baseline performance.\n",
    "\n",
    "## Objectives:\n",
    "- Train U-Net model on original data only\n",
    "- Establish baseline performance metrics\n",
    "- Save model for comparison with augmented version\n",
    "- Monitor training progress and prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "from data_preprocessing import DataPreprocessor\n",
    "from model_architecture import create_model, initialize_weights\n",
    "from training import ColorizationTrainer, train_model\n",
    "from evaluation import ColorizationEvaluator, evaluate_single_image\n",
    "from utils import create_training_curves, save_experiment_config, log_gpu_usage\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    log_gpu_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config_path = \"../config/config.yaml\"\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"Baseline Training Configuration:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Model architecture: {config['model']['architecture']}\")\n",
    "print(f\"Input size: {config['data']['input_size']}\")\n",
    "print(f\"Batch size: {config['data']['batch_size']}\")\n",
    "print(f\"Learning rate: {config['training']['learning_rate']}\")\n",
    "print(f\"Epochs: {config['training']['epochs']}\")\n",
    "print(f\"Loss function: {config['training']['loss_function']}\")\n",
    "\n",
    "# Setup directories\n",
    "processed_dir = \"../data/processed\"\n",
    "train_dir = os.path.join(processed_dir, \"train\")\n",
    "val_dir = os.path.join(processed_dir, \"val\")\n",
    "test_dir = os.path.join(processed_dir, \"test\")\n",
    "\n",
    "baseline_model_dir = \"../models/baseline_model\"\n",
    "baseline_results_dir = \"../results/baseline\"\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(baseline_model_dir, exist_ok=True)\n",
    "os.makedirs(baseline_results_dir, exist_ok=True)\n",
    "\n",
    "print(f\"\\nModel will be saved to: {baseline_model_dir}\")\n",
    "print(f\"Results will be saved to: {baseline_results_dir}\")\n",
    "\n",
    "# Check data availability\n",
    "def count_images(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        return 0\n",
    "    return len([f for f in os.listdir(directory) \n",
    "               if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "\n",
    "train_count = count_images(train_dir)\n",
    "val_count = count_images(val_dir)\n",
    "test_count = count_images(test_dir)\n",
    "\n",
    "print(f\"\\nDataset Status:\")\n",
    "print(f\"Training images: {train_count}\")\n",
    "print(f\"Validation images: {val_count}\")\n",
    "print(f\"Test images: {test_count}\")\n",
    "\n",
    "if train_count == 0:\n",
    "    print(\"\\n❌ No training data found!\")\n",
    "    print(\"Please run notebook 02_data_preprocessing.ipynb first.\")\n",
    "    raise SystemExit(\"Training data required\")\n",
    "\n",
    "if val_count == 0:\n",
    "    print(\"\\n⚠️ No validation data found. Using training data for validation.\")\n",
    "    val_dir = train_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Data Loaders (No Augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data preprocessor\n",
    "preprocessor = DataPreprocessor(config_path)\n",
    "\n",
    "print(\"Creating data loaders (baseline - no augmentation)...\")\n",
    "\n",
    "try:\n",
    "    # Create data loaders without augmentation\n",
    "    train_loader, val_loader, test_loader = preprocessor.create_dataloaders(\n",
    "        train_dir,\n",
    "        val_dir,\n",
    "        test_dir if test_count > 0 else None\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ Data loaders created successfully!\")\n",
    "    print(f\"  Train batches: {len(train_loader)}\")\n",
    "    print(f\"  Validation batches: {len(val_loader)}\")\n",
    "    if test_loader:\n",
    "        print(f\"  Test batches: {len(test_loader)}\")\n",
    "    \n",
    "    # Test data loading\n",
    "    print(\"\\nTesting data loading...\")\n",
    "    for L, AB, filenames in train_loader:\n",
    "        print(f\"✓ Batch loaded successfully:\")\n",
    "        print(f\"  L channel shape: {L.shape}\")\n",
    "        print(f\"  AB channels shape: {AB.shape}\")\n",
    "        print(f\"  Value ranges: L[{L.min():.3f}, {L.max():.3f}], AB[{AB.min():.3f}, {AB.max():.3f}]\")\n",
    "        break\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error creating data loaders: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create and Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create baseline model\n",
    "print(\"Creating baseline model...\")\n",
    "\n",
    "try:\n",
    "    model = create_model(config_path)\n",
    "    \n",
    "    # Initialize weights\n",
    "    initialize_weights(model)\n",
    "    \n",
    "    # Print model information\n",
    "    num_params = model.count_parameters()\n",
    "    model_size = model.get_model_size()\n",
    "    \n",
    "    print(f\"✓ Model created successfully!\")\n",
    "    print(f\"  Architecture: {model.architecture}\")\n",
    "    print(f\"  Parameters: {num_params:,}\")\n",
    "    print(f\"  Model size: {model_size:.2f} MB\")\n",
    "    \n",
    "    # Test forward pass\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    print(f\"  Device: {device}\")\n",
    "    \n",
    "    # Test with dummy input\n",
    "    dummy_input = torch.randn(1, 1, *config['data']['input_size']).to(device)\n",
    "    with torch.no_grad():\n",
    "        dummy_output = model(dummy_input)\n",
    "    \n",
    "    print(f\"  Test forward pass: ✓\")\n",
    "    print(f\"  Input shape: {dummy_input.shape}\")\n",
    "    print(f\"  Output shape: {dummy_output.shape}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error creating model: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "print(\"Setting up baseline training...\")\n",
    "\n",
    "try:\n",
    "    trainer = ColorizationTrainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        config_path=config_path\n",
    "    )\n",
    "    \n",
    "    print(\"✓ Trainer initialized successfully!\")\n",
    "    print(f\"  Device: {trainer.device}\")\n",
    "    print(f\"  Mixed precision: {trainer.use_amp}\")\n",
    "    print(f\"  Optimizer: {type(trainer.optimizer).__name__}\")\n",
    "    print(f\"  Scheduler: {type(trainer.scheduler).__name__ if trainer.scheduler else 'None'}\")\n",
    "    print(f\"  Loss function: {type(trainer.criterion).__name__}\")\n",
    "    \n",
    "    # Log GPU usage after model creation\n",
    "    if torch.cuda.is_available():\n",
    "        log_gpu_usage()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error setting up training: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Progress Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup monitoring\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "def plot_training_progress(train_losses, val_losses, title=\"Baseline Model Training Progress\"):\n",
    "    \"\"\"Plot training progress in real-time.\"\"\"\n",
    "    if len(train_losses) == 0:\n",
    "        return\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # Loss curves\n",
    "    plt.subplot(1, 2, 1)\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    plt.plot(epochs, train_losses, 'b-', label='Training Loss', linewidth=2)\n",
    "    plt.plot(epochs, val_losses, 'r-', label='Validation Loss', linewidth=2)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training vs Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Recent loss (last 20 epochs)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    recent_start = max(0, len(train_losses) - 20)\n",
    "    recent_epochs = epochs[recent_start:]\n",
    "    recent_train = train_losses[recent_start:]\n",
    "    recent_val = val_losses[recent_start:]\n",
    "    \n",
    "    plt.plot(recent_epochs, recent_train, 'b-', label='Training Loss', linewidth=2)\n",
    "    plt.plot(recent_epochs, recent_val, 'r-', label='Validation Loss', linewidth=2)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Recent Training Progress (Last 20 Epochs)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print current status\n",
    "    current_epoch = len(train_losses)\n",
    "    current_train_loss = train_losses[-1]\n",
    "    current_val_loss = val_losses[-1]\n",
    "    best_val_loss = min(val_losses)\n",
    "    \n",
    "    print(f\"Epoch {current_epoch}: Train Loss = {current_train_loss:.4f}, Val Loss = {current_val_loss:.4f}\")\n",
    "    print(f\"Best Validation Loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        memory_used = torch.cuda.memory_allocated() / 1024**3\n",
    "        print(f\"GPU Memory: {memory_used:.2f} GB\")\n",
    "\n",
    "print(\"Training monitoring setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Start Baseline Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save experiment configuration\n",
    "experiment_name = f\"baseline_training_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "save_experiment_config(config, experiment_name, baseline_results_dir)\n",
    "\n",
    "print(f\"Starting baseline model training...\")\n",
    "print(f\"Experiment name: {experiment_name}\")\n",
    "print(f\"Results directory: {baseline_results_dir}\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"BASELINE TRAINING (NO AUGMENTATION)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "try:\n",
    "    # Start training\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Custom training loop with progress monitoring\n",
    "    num_epochs = config['training']['epochs']\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    print(f\"Training for {num_epochs} epochs...\")\n",
    "    \n",
    "    # Option 1: Use built-in trainer (simpler)\n",
    "    history = trainer.train()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    training_duration = end_time - start_time\n",
    "    \n",
    "    print(f\"\\n✅ Baseline training completed!\")\n",
    "    print(f\"Training time: {training_duration:.2f} seconds ({training_duration/60:.1f} minutes)\")\n",
    "    print(f\"Best validation loss: {history['best_val_loss']:.4f}\")\n",
    "    print(f\"Final training loss: {history['train_losses'][-1]:.4f}\")\n",
    "    print(f\"Final validation loss: {history['val_losses'][-1]:.4f}\")\n",
    "    \n",
    "    # Save training history\n",
    "    history_path = os.path.join(baseline_results_dir, 'training_history.yaml')\n",
    "    with open(history_path, 'w') as f:\n",
    "        yaml.dump({\n",
    "            'experiment_name': experiment_name,\n",
    "            'training_duration_seconds': training_duration,\n",
    "            'best_val_loss': float(history['best_val_loss']),\n",
    "            'final_train_loss': float(history['train_losses'][-1]),\n",
    "            'final_val_loss': float(history['val_losses'][-1]),\n",
    "            'total_epochs': len(history['train_losses']),\n",
    "            'train_losses': [float(x) for x in history['train_losses']],\n",
    "            'val_losses': [float(x) for x in history['val_losses']]\n",
    "        }, f)\n",
    "    \n",
    "    print(f\"Training history saved to: {history_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Training failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained baseline model\n",
    "if 'history' in locals():\n",
    "    print(\"Saving baseline model...\")\n",
    "    \n",
    "    try:\n",
    "        # Save complete model state\n",
    "        model_save_path = os.path.join(baseline_model_dir, 'baseline_model_complete.pth')\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'model_architecture': config['model']['architecture'],\n",
    "            'model_config': config['model'],\n",
    "            'training_config': config['training'],\n",
    "            'data_config': config['data'],\n",
    "            'training_history': history,\n",
    "            'experiment_name': experiment_name,\n",
    "            'total_parameters': model.count_parameters(),\n",
    "            'model_size_mb': model.get_model_size()\n",
    "        }, model_save_path)\n",
    "        \n",
    "        # Save just the model weights (for easy loading)\n",
    "        weights_save_path = os.path.join(baseline_model_dir, 'baseline_weights.pth')\n",
    "        torch.save(model.state_dict(), weights_save_path)\n",
    "        \n",
    "        print(f\"✓ Complete model saved to: {model_save_path}\")\n",
    "        print(f\"✓ Model weights saved to: {weights_save_path}\")\n",
    "        \n",
    "        # Create model info file\n",
    "        model_info = {\n",
    "            'model_name': 'Baseline Colorization Model',\n",
    "            'architecture': config['model']['architecture'],\n",
    "            'input_channels': config['model']['input_channels'],\n",
    "            'output_channels': config['model']['output_channels'],\n",
    "            'total_parameters': model.count_parameters(),\n",
    "            'model_size_mb': model.get_model_size(),\n",
    "            'training_data_augmented': False,\n",
    "            'best_validation_loss': float(history['best_val_loss']),\n",
    "            'training_epochs': len(history['train_losses']),\n",
    "            'training_duration_minutes': training_duration / 60,\n",
    "            'created_date': datetime.now().isoformat(),\n",
    "            'files': {\n",
    "                'complete_model': 'baseline_model_complete.pth',\n",
    "                'weights_only': 'baseline_weights.pth',\n",
    "                'training_history': '../results/baseline/training_history.yaml'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        info_path = os.path.join(baseline_model_dir, 'model_info.yaml')\n",
    "        with open(info_path, 'w') as f:\n",
    "            yaml.dump(model_info, f, default_flow_style=False)\n",
    "        \n",
    "        print(f\"✓ Model info saved to: {info_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error saving model: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"No training history available - model not saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Generate Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and save training curves\n",
    "if 'history' in locals():\n",
    "    print(\"Generating training curves...\")\n",
    "    \n",
    "    try:\n",
    "        # Create training curves\n",
    "        curves_path = os.path.join(baseline_results_dir, 'training_curves.png')\n",
    "        create_training_curves(\n",
    "            history['train_losses'],\n",
    "            history['val_losses'],\n",
    "            save_path=curves_path,\n",
    "            title=\"Baseline Model Training Curves (No Augmentation)\"\n",
    "        )\n",
    "        \n",
    "        print(f\"✓ Training curves saved to: {curves_path}\")\n",
    "        \n",
    "        # Display final training statistics\n",
    "        print(\"\\nFinal Training Statistics:\")\n",
    "        print(\"=\" * 40)\n",
    "        print(f\"Total epochs: {len(history['train_losses'])}\")\n",
    "        print(f\"Best validation loss: {history['best_val_loss']:.4f}\")\n",
    "        print(f\"Final training loss: {history['train_losses'][-1]:.4f}\")\n",
    "        print(f\"Final validation loss: {history['val_losses'][-1]:.4f}\")\n",
    "        print(f\"Training time: {training_duration/60:.1f} minutes\")\n",
    "        print(f\"Model parameters: {model.count_parameters():,}\")\n",
    "        print(f\"Model size: {model.get_model_size():.2f} MB\")\n",
    "        \n",
    "        # Check for overfitting\n",
    "        final_train_loss = history['train_losses'][-1]\n",
    "        final_val_loss = history['val_losses'][-1]\n",
    "        loss_gap = final_val_loss - final_train_loss\n",
    "        \n",
    "        print(f\"\\nOverfitting Analysis:\")\n",
    "        print(f\"Train-Validation gap: {loss_gap:.4f}\")\n",
    "        \n",
    "        if loss_gap > 0.01:\n",
    "            print(\"⚠️  Potential overfitting detected (validation loss > training loss)\")\n",
    "            print(\"   Consider: regularization, more data, or data augmentation\")\n",
    "        elif loss_gap < -0.01:\n",
    "            print(\"⚠️  Unusual: training loss > validation loss\")\n",
    "            print(\"   This might indicate: different data distributions or model issues\")\n",
    "        else:\n",
    "            print(\"✓ Good train-validation balance\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error generating training curves: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"No training history available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Quick Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform quick evaluation on validation set\n",
    "if 'model' in locals() and 'val_loader' in locals():\n",
    "    print(\"Performing quick baseline model evaluation...\")\n",
    "    \n",
    "    try:\n",
    "        evaluator = ColorizationEvaluator(config_path)\n",
    "        \n",
    "        # Evaluate on a subset of validation data for speed\n",
    "        print(\"Evaluating on validation set (subset)...\")\n",
    "        \n",
    "        # Take first few batches for quick evaluation\n",
    "        quick_metrics = {'psnr': [], 'ssim': [], 'mse': [], 'mae': []}\n",
    "        samples_evaluated = 0\n",
    "        max_samples = 50  # Limit for quick evaluation\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (L, AB, filenames) in enumerate(val_loader):\n",
    "                if samples_evaluated >= max_samples:\n",
    "                    break\n",
    "                    \n",
    "                batch_metrics = evaluator.evaluate_batch(model, L, AB, filenames)\n",
    "                \n",
    "                for key in quick_metrics:\n",
    "                    quick_metrics[key].append(batch_metrics[key])\n",
    "                \n",
    "                samples_evaluated += len(filenames)\n",
    "                \n",
    "                if batch_idx % 5 == 0:\n",
    "                    print(f\"  Evaluated {samples_evaluated} samples...\")\n",
    "        \n",
    "        # Calculate average metrics\n",
    "        avg_metrics = {key: np.mean(values) for key, values in quick_metrics.items()}\n",
    "        \n",
    "        print(f\"\\nBaseline Model Quick Evaluation Results:\")\n",
    "        print(\"=\" * 45)\n",
    "        print(f\"Samples evaluated: {samples_evaluated}\")\n",
    "        print(f\"PSNR: {avg_metrics['psnr']:.2f} dB\")\n",
    "        print(f\"SSIM: {avg_metrics['ssim']:.4f}\")\n",
    "        print(f\"MSE: {avg_metrics['mse']:.6f}\")\n",
    "        print(f\"MAE: {avg_metrics['mae']:.6f}\")\n",
    "        \n",
    "        # Save quick evaluation results\n",
    "        eval_results = {\n",
    "            'model_type': 'baseline',\n",
    "            'augmentation_used': False,\n",
    "            'samples_evaluated': samples_evaluated,\n",
    "            'quick_evaluation': True,\n",
    "            'metrics': avg_metrics,\n",
    "            'evaluation_date': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        eval_path = os.path.join(baseline_results_dir, 'quick_evaluation.yaml')\n",
    "        with open(eval_path, 'w') as f:\n",
    "            yaml.dump(eval_results, f, default_flow_style=False)\n",
    "        \n",
    "        print(f\"\\n✓ Quick evaluation results saved to: {eval_path}\")\n",
    "        \n",
    "        # Performance interpretation\n",
    "        print(\"\\nPerformance Interpretation:\")\n",
    "        print(\"-\" * 25)\n",
    "        \n",
    "        if avg_metrics['psnr'] > 25:\n",
    "            print(\"✅ PSNR: Excellent (> 25 dB)\")\n",
    "        elif avg_metrics['psnr'] > 20:\n",
    "            print(\"✅ PSNR: Good (20-25 dB)\")\n",
    "        elif avg_metrics['psnr'] > 15:\n",
    "            print(\"⚠️  PSNR: Fair (15-20 dB)\")\n",
    "        else:\n",
    "            print(\"❌ PSNR: Poor (< 15 dB)\")\n",
    "        \n",
    "        if avg_metrics['ssim'] > 0.8:\n",
    "            print(\"✅ SSIM: Excellent (> 0.8)\")\n",
    "        elif avg_metrics['ssim'] > 0.6:\n",
    "            print(\"✅ SSIM: Good (0.6-0.8)\")\n",
    "        elif avg_metrics['ssim'] > 0.4:\n",
    "            print(\"⚠️  SSIM: Fair (0.4-0.6)\")\n",
    "        else:\n",
    "            print(\"❌ SSIM: Poor (< 0.4)\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during evaluation: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"Model or validation data not available for evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Generate Sample Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample colorization results\n",
    "if 'model' in locals() and 'val_loader' in locals():\n",
    "    print(\"Generating sample colorization results...\")\n",
    "    \n",
    "    try:\n",
    "        # Get a few sample images\n",
    "        model.eval()\n",
    "        sample_results = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (L, AB, filenames) in enumerate(val_loader):\n",
    "                if batch_idx >= 2:  # Only take first 2 batches\n",
    "                    break\n",
    "                \n",
    "                # Get model predictions\n",
    "                L_gpu = L.to(device)\n",
    "                AB_pred = model(L_gpu).cpu()\n",
    "                \n",
    "                # Convert to RGB for visualization\n",
    "                pred_rgb = evaluator.lab_to_rgb(L, AB_pred)\n",
    "                target_rgb = evaluator.lab_to_rgb(L, AB)\n",
    "                \n",
    "                # Store results\n",
    "                for i in range(min(4, len(filenames))):\n",
    "                    sample_results.append({\n",
    "                        'filename': filenames[i],\n",
    "                        'input_L': L[i, 0].numpy(),\n",
    "                        'prediction': pred_rgb[i],\n",
    "                        'target': target_rgb[i]\n",
    "                    })\n",
    "        \n",
    "        # Create visualization\n",
    "        if sample_results:\n",
    "            num_samples = min(6, len(sample_results))\n",
    "            fig, axes = plt.subplots(3, num_samples, figsize=(3*num_samples, 9))\n",
    "            \n",
    "            if num_samples == 1:\n",
    "                axes = axes.reshape(3, 1)\n",
    "            \n",
    "            for i in range(num_samples):\n",
    "                result = sample_results[i]\n",
    "                \n",
    "                # Denormalize L channel for display\n",
    "                L_display = (result['input_L'] + 1) / 2  # -1,1 -> 0,1\n",
    "                \n",
    "                # Input (grayscale)\n",
    "                axes[0, i].imshow(L_display, cmap='gray')\n",
    "                axes[0, i].set_title(f'Input\\n{result[\"filename\"][:15]}...', fontsize=10)\n",
    "                axes[0, i].axis('off')\n",
    "                \n",
    "                # Prediction\n",
    "                axes[1, i].imshow(result['prediction'])\n",
    "                axes[1, i].set_title('Baseline Prediction', fontsize=10)\n",
    "                axes[1, i].axis('off')\n",
    "                \n",
    "                # Target\n",
    "                axes[2, i].imshow(result['target'])\n",
    "                axes[2, i].set_title('Ground Truth', fontsize=10)\n",
    "                axes[2, i].axis('off')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save sample results\n",
    "            samples_path = os.path.join(baseline_results_dir, 'sample_results.png')\n",
    "            plt.savefig(samples_path, dpi=150, bbox_inches='tight')\n",
    "            plt.show()\n",
    "            \n",
    "            print(f\"✓ Sample results saved to: {samples_path}\")\n",
    "        \n",
    "        else:\n",
    "            print(\"No sample results generated\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error generating sample results: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"Model or data not available for generating sample results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comprehensive summary\n",
    "print(\"BASELINE TRAINING SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if 'history' in locals():\n",
    "    print(f\"✅ Training Status: COMPLETED\")\n",
    "    print(f\"📊 Final Validation Loss: {history['best_val_loss']:.4f}\")\n",
    "    print(f\"⏱️  Training Duration: {training_duration/60:.1f} minutes\")\n",
    "    print(f\"🔢 Model Parameters: {model.count_parameters():,}\")\n",
    "    print(f\"💾 Model Size: {model.get_model_size():.2f} MB\")\n",
    "    \n",
    "    if 'avg_metrics' in locals():\n",
    "        print(f\"\\n📈 Quick Evaluation Metrics:\")\n",
    "        print(f\"   PSNR: {avg_metrics['psnr']:.2f} dB\")\n",
    "        print(f\"   SSIM: {avg_metrics['ssim']:.4f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Training Status: NOT COMPLETED\")\n",
    "\n",
    "print(f\"\\n📁 Files Generated:\")\n",
    "generated_files = [\n",
    "    f\"models/baseline_model/baseline_model_complete.pth\",\n",
    "    f\"models/baseline_model/baseline_weights.pth\",\n",
    "    f\"models/baseline_model/model_info.yaml\",\n",
    "    f\"results/baseline/training_history.yaml\",\n",
    "    f\"results/baseline/training_curves.png\",\n",
    "    f\"results/baseline/quick_evaluation.yaml\",\n",
    "    f\"results/baseline/sample_results.png\"\n",
    "]\n",
    "\n",
    "for file in generated_files:\n",
    "    if os.path.exists(f\"../{file}\"):\n",
    "        print(f\"   ✅ {file}\")\n",
    "    else:\n",
    "        print(f\"   ❌ {file} (not created)\")\n",
    "\n",
    "print(f\"\\n🎯 Next Steps:\")\n",
    "print(f\"1. ✅ Baseline model training completed\")\n",
    "print(f\"2. 📝 Run notebook 05_model_training_augmented.ipynb to train with augmentation\")\n",
    "print(f\"3. 📊 Run notebook 06_evaluation_comparison.ipynb to compare baseline vs augmented\")\n",
    "print(f\"4. 🖥️  Use notebook 07_gui_demo.ipynb to test the trained model interactively\")\n",
    "\n",
    "print(f\"\\n💡 Key Observations:\")\n",
    "if 'history' in locals():\n",
    "    final_gap = history['val_losses'][-1] - history['train_losses'][-1]\n",
    "    if final_gap > 0.01:\n",
    "        print(f\"   ⚠️  Model shows signs of overfitting (val loss > train loss)\")\n",
    "        print(f\"   🔧 Data augmentation should help improve generalization\")\n",
    "    else:\n",
    "        print(f\"   ✅ Model shows good train-validation balance\")\n",
    "        print(f\"   📈 Augmentation may still improve performance on diverse data\")\n",
    "        \n",
    "    if 'avg_metrics' in locals() and avg_metrics['psnr'] < 20:\n",
    "        print(f\"   📊 PSNR could be improved - try different architectures or longer training\")\n",
    "else:\n",
    "    print(f\"   ❌ Complete the training first to get observations\")\n",
    "    \n",
    "print(f\"\\n🏁 Baseline training phase complete! Ready for augmentation experiments.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
